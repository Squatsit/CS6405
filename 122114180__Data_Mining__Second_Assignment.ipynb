{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Squatsit/CS6405/blob/main/122114180__Data_Mining__Second_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CS3033/CS6405 - Data Mining - Second Assignment\n",
        "\n",
        "### Submission\n",
        "\n",
        "This assignment is **due on 07/04/22 at 23:59**. You should submit a single .ipnyb file with your python code and analysis electronically via Canvas.\n",
        "Please note that this assignment will account for 25 Marks of your module grade.\n",
        "\n",
        "### Declaration\n",
        "\n",
        "By submitting this assignment. I agree to the following:\n",
        "\n",
        "<font color=\"red\">“I have read and understand the UCC academic policy on plagiarism, and agree to the requirements set out thereby in relation to plagiarism and referencing. I confirm that I have referenced and acknowledged properly all sources used in the preparation of this assignment.\n",
        "I declare that this assignment is entirely my own work based on my personal study. I further declare that I have not engaged the services of another to either assist me in, or complete this assignment”</font>\n",
        "\n",
        "### Objective\n",
        "\n",
        "The Boolean satisfiability (SAT) problem consists in determining whether a Boolean formula F is satisfiable or not. F is represented by a pair (X, C), where X is a set of Boolean variables and C is a set of clauses in Conjunctive Normal Form (CNF). Each clause is a disjunction of literals (a variable or its negation). This problem is one of the most widely studied combinatorial problems in computer science. It is the classic NP-complete problem. Over the past number of decades, a significant amount of research work has focused on solving SAT problems with both complete and incomplete solvers.\n",
        "\n",
        "One of the most successful approaches is an algorithm portfolio, where a solver is selected among a set of candidates depending on the problem type. Your task is to create a classifier that takes as input the SAT instance's features and identifies the class.\n",
        "\n",
        "In this project, we represent SAT problems with a vector of 327 features with general information about the problem, e.g., number of variables, number of clauses, the fraction of horn clauses in the problem, etc. There is no need to understand the features to be able to complete the assignment.\n",
        "\n",
        "\n",
        "The original dataset is available at:\n",
        "https://github.com/bprovanbessell/SATfeatPy/blob/main/features_csv/all_features.csv\n",
        "\n"
      ],
      "metadata": {
        "id": "8WfrCFmLHxYu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation"
      ],
      "metadata": {
        "id": "Oav9G1WSJ1nH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/andvise/DataAnalyticsDatasets/main/train_dataset.csv\", index_col=0)\n",
        "old_shape=df.shape\n",
        "old_shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DE0kM0QsJ1En",
        "outputId": "f16cea41-deb2-4fb8-89a3-911ab05edc21"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2412, 328)"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Label or target variable\n",
        "df['target'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8MCvTYTKw4Q",
        "outputId": "cede3833-01db-4b21-e875-bc228cd72aaf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tseitin           298\n",
              "dominating        294\n",
              "cliquecoloring    268\n",
              "php               266\n",
              "subsetcard        263\n",
              "op                201\n",
              "tiling            120\n",
              "5clique           108\n",
              "3color            104\n",
              "matching          102\n",
              "5color             98\n",
              "4color             98\n",
              "3clique            98\n",
              "4clique            94\n",
              "Name: target, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tasks\n",
        "\n",
        "## Basic models and evaluation (5 Marks)\n",
        "\n",
        "Using Scikit-learn, train and evaluate a decision tree classifier using 70% of the dataset from training and 30% for testing. For this part of the project, we are not interested in optimising the parameters; we just want to get an idea of the dataset."
      ],
      "metadata": {
        "id": "MTvkBPQvITf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#When I tried to apply scaling there was an error regarding na and inf\n",
        "\n",
        "#To remove na/infinity values\n",
        "print('Na Values: ',df.isnull().values.any()) #True\n",
        "\n",
        "df=df.dropna(axis=1) \n",
        "#originally tried rows but all were removed\n",
        "\n",
        "import numpy as np\n",
        "print('Infinity Values:',df.isin([np.inf, -np.inf]).values.any()) #True\n",
        "\n",
        "#replace with na and then drop these rows\n",
        "df = df.replace([np.inf, -np.inf], np.nan).dropna(axis=1) \n",
        "\n",
        "print('Sanity Check:',df.isnull().values.any())\n",
        "\n",
        "print('Rows Lost =',old_shape[0]-df.shape[0],'Columns Lost =',old_shape[1]-df.shape[1] )"
      ],
      "metadata": {
        "id": "Wb3GRjoZ9x_A",
        "outputId": "8373606c-9321-4741-a363-24388a2b4da2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Na Values:  True\n",
            "Infinity Values: True\n",
            "Sanity Check: False\n",
            "Rows Lost = 0 Columns Lost = 18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE\n",
        "df_target=df['target']\n",
        "df=df.drop(columns=['target'])\n",
        "\n",
        "from sklearn import model_selection\n",
        "train_features,test_features,train_labels,test_labels=model_selection.train_test_split(df,df_target,test_size=0.3, random_state=17)\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler= MinMaxScaler()\n",
        "train_features = scaler.fit_transform(train_features)\n",
        "test_features = scaler.transform(test_features)\n",
        "\n",
        "from sklearn import preprocessing\n",
        "le = preprocessing.LabelEncoder()\n",
        "train_labels=le.fit_transform(train_labels)\n",
        "test_labels=le.transform(test_labels)\n",
        "#used PCA to reduce dimensionality of data as 312 is a lot of features\n",
        "#used n=8 arbitraroly was in the lab\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=8)\n",
        "train_features_pca=pca.fit_transform(train_features)\n",
        "test_features_pca=pca.transform(test_features)\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "clf1 = DecisionTreeClassifier(random_state=0)\n",
        "clf1.fit(train_features_pca,train_labels)\n",
        "results=clf1.predict(test_features_pca)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(test_labels, results)"
      ],
      "metadata": {
        "id": "Zl0VXO0YH1nG",
        "outputId": "b757932b-e2a3-4605-8cc8-d2ddef368ebc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.93646408839779"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_features.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcFQ3nuqPSXh",
        "outputId": "e1d0457e-94ee-48b7-ae59-8f9a33ed2bce"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1688, 309)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Robust evaluation (10 Marks)\n",
        "\n",
        "In this section, we are interested in more rigorous techniques by implementing more sophisticated methods, for instance:\n",
        "* Hold-out and cross-validation.\n",
        "* Hyper-parameter tuning.\n",
        "* Feature reduction.\n",
        "* Feature selection.\n",
        "* Feature normalisation.\n",
        "\n",
        "Your report should provide concrete information about your reasoning; everything should be well-explained.\n",
        "\n",
        "The key to geting good marks is to show that you evaluated different methods and that you correctly selected the configuration."
      ],
      "metadata": {
        "id": "zADpr0f8IcGL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Reduction\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.decomposition import PCA\n",
        "#PCA\n",
        "PCA_classifierDT = Pipeline([\n",
        "    ('pca',PCA()),\n",
        "    (\"predictor\", DecisionTreeClassifier(random_state=0))\n",
        "    ])\n",
        "\n",
        "param_grid = {\"pca__n_components\": [8,10,12], #10,20,30 --> then tried 5,7,9,10,12 8,9,10,11 \n",
        "              \"predictor__criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n",
        "              \"predictor__splitter\": [\"best\",\"random\"],\n",
        "              \"predictor__max_features\":[\"sqrt\", \"log2\",None]\n",
        "              }\n",
        "\n",
        "#cross validation\n",
        "DT_gs = GridSearchCV(PCA_classifierDT, param_grid, scoring=\"accuracy\",cv=10)\n",
        "\n",
        "# Run the GridSearchCV\n",
        "DT_gs.fit(train_features, train_labels)\n",
        "\n",
        "# Print the best parameters and the score\n",
        "DT_gs.best_params_, DT_gs.best_score_"
      ],
      "metadata": {
        "id": "tvBZH6ilInsA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c4efc64-2b21-495e-d62b-68075321db9a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'pca__n_components': 10,\n",
              "  'predictor__criterion': 'entropy',\n",
              "  'predictor__max_features': None,\n",
              "  'predictor__splitter': 'random'},\n",
              " 0.9543850380388841)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Final Test on unseen data\n",
        "PCA_classifierDT.set_params(**DT_gs.best_params_) \n",
        "PCA_classifierDT.fit(train_features, train_labels)\n",
        "accuracy_score(test_labels, PCA_classifierDT.predict(test_features))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "huVDLi3m0005",
        "outputId": "69295bd6-12c8-463f-9db8-e0567ee6bdd0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9488950276243094"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Selection\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import f_classif\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "\n",
        "\n",
        "#Remove all features which have a variance below 0.03\n",
        "constant_filter = VarianceThreshold(threshold=0.03)\n",
        "train_features0=constant_filter.fit_transform(train_features)\n",
        "test_features0=constant_filter.transform(test_features)\n",
        "\n",
        "\n",
        "FS_classifierDT = Pipeline([\n",
        "    (\"feature_selection\",SelectKBest(f_classif)),\n",
        "    (\"predictor\", DecisionTreeClassifier(random_state=0))\n",
        "    ])\n",
        "\n",
        "param_grid = {\"feature_selection__k\": [20,40,80,'all'], #10,20,30 --> then tried 5,7,9,10,12 8,9,10,11 \n",
        "              \"predictor__criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n",
        "              \"predictor__splitter\": [\"best\",\"random\"],\n",
        "              \"predictor__max_features\":[\"sqrt\", \"log2\",None]\n",
        "              }\n",
        "\n",
        "#cross validation\n",
        "FS_DT_gs = GridSearchCV(FS_classifierDT, param_grid, scoring=\"accuracy\",cv=10)\n",
        "\n",
        "# Run the GridSearchCV\n",
        "FS_DT_gs.fit(train_features0, train_labels)\n",
        "\n",
        "# Print the best parameters and the score\n",
        "FS_DT_gs.best_params_, FS_DT_gs.best_score_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVhYEUo4gAdP",
        "outputId": "f072c27a-f8b9-4d91-889b-3c178413e656"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'feature_selection__k': 40,\n",
              "  'predictor__criterion': 'entropy',\n",
              "  'predictor__max_features': None,\n",
              "  'predictor__splitter': 'random'},\n",
              " 0.9846012961397577)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_features0.shape \n",
        "#Feature reduced from 309 to 127 to 80"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POF2ieOjMxe7",
        "outputId": "51ecb33d-42d6-4d0d-e978-7dd39fc6b603"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1688, 130)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Final Test on Unseen Data\n",
        "FS_classifierDT.set_params(**FS_DT_gs.best_params_) \n",
        "FS_classifierDT.fit(train_features0, train_labels)\n",
        "accuracy_score(test_labels, FS_classifierDT.predict(test_features0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVyhqCdTbvjB",
        "outputId": "e0f13752-8774-4244-ddf0-1cf46cb56d7f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9917127071823204"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## New classifier (10 Marks)\n",
        "\n",
        "Replicate the previous task for a classifier different than K-NN and decision trees. Briefly describe your choice.\n",
        "Try to create the best model for the given dataset.\n",
        "\n",
        "\n",
        "Save your best model into your github. And create a single code cell that loads it and evaluate it on the following test dataset:\n",
        "https://github.com/andvise/DataAnalyticsDatasets/blob/main/test_dataset.csv\n",
        "\n",
        "This link currently contains a sample of the training set. The real test set will be released after the submission. I should be able to run the code cell independently, load all the libraries you need as well."
      ],
      "metadata": {
        "id": "FYoMg0EZIrNd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE\n",
        "import numpy as np\n",
        "df = pd.read_csv(\"https://github.com/andvise/DataAnalyticsDatasets/blob/main/test_dataset.csv?raw=True\",index_col=0)\n",
        "old_shape=df.shape\n",
        "\n",
        "print('Na Values: ',df.isnull().values.any()) #True\n",
        "\n",
        "df=df.dropna(axis=1) \n",
        "#originally tried rows but all were removed\n",
        "\n",
        "print('Infinity Values:',df.isin([np.inf, -np.inf]).values.any()) #True\n",
        "\n",
        "#replace with na and then drop these rows\n",
        "df = df.replace([np.inf, -np.inf], np.nan).dropna(axis=1) \n",
        "\n",
        "print('Sanity Check:',df.isnull().values.any())\n",
        "\n",
        "print('Rows Lost =',old_shape[0]-df.shape[0],'Columns Lost =',old_shape[1]-df.shape[1] )\n",
        "\n",
        "df_target=df['target']\n",
        "df=df.drop(columns=['target'])\n",
        "\n",
        "from sklearn import model_selection\n",
        "train_features,test_features,train_labels,test_labels=model_selection.train_test_split(df,df_target,test_size=0.3, random_state=17)\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler= MinMaxScaler()\n",
        "train_features = scaler.fit_transform(train_features)\n",
        "test_features = scaler.transform(test_features)\n",
        "\n",
        "from sklearn import preprocessing\n",
        "le = preprocessing.LabelEncoder()\n",
        "train_labels=le.fit_transform(train_labels)\n",
        "test_labels=le.transform(test_labels)\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "PCA_classifierRF = Pipeline([\n",
        "    ('pca',PCA()),\n",
        "    (\"predictor\", RandomForestClassifier())\n",
        "    ])\n",
        "\n",
        "param_grid = {\"pca__n_components\": [10,12,14], #5,10,15,20 --> 15 --> 12,14,16,18\n",
        "              \"predictor__n_estimators\": [250,275],\n",
        "              \"predictor__criterion\": [\"gini\", \"log_loss\"],\n",
        "              \"predictor__max_features\":[\"sqrt\", \"log2\", None]\n",
        "              }\n",
        "\n",
        "#cross validation\n",
        "RF_gs = GridSearchCV(PCA_classifierRF, param_grid, scoring=\"accuracy\",cv=5)\n",
        "\n",
        "# Run the GridSearchCV\n",
        "RF_gs.fit(train_features, train_labels)\n",
        "\n",
        "# Print the best parameters and the score\n",
        "RF_gs.best_params_, RF_gs.best_score_"
      ],
      "metadata": {
        "id": "QRJXrY2hI32F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6d5d7a2-3f19-49d1-e6b6-10de96731b28"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Na Values:  True\n",
            "Infinity Values: True\n",
            "Sanity Check: False\n",
            "Rows Lost = 0 Columns Lost = 16\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'pca__n_components': 14,\n",
              "  'predictor__criterion': 'gini',\n",
              "  'predictor__max_features': 'log2',\n",
              "  'predictor__n_estimators': 275},\n",
              " 0.93801580333626)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PCA_classifierRF.set_params(**RF_gs.best_params_) \n",
        "PCA_classifierRF.fit(train_features, train_labels)\n",
        "accuracy_score(test_labels, PCA_classifierRF.predict(test_features))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-48HUtV0-off",
        "outputId": "b0ff12dd-3c91-4873-dc6f-ab85df44221b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9726027397260274"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "df = pd.read_csv(\"https://github.com/andvise/DataAnalyticsDatasets/blob/main/test_dataset.csv?raw=True\",index_col=0)\n",
        "old_shape=df.shape\n",
        "\n",
        "print('Na Values: ',df.isnull().values.any()) #True\n",
        "\n",
        "df=df.dropna(axis=1) \n",
        "#originally tried rows but all were removed\n",
        "\n",
        "print('Infinity Values:',df.isin([np.inf, -np.inf]).values.any()) #True\n",
        "\n",
        "#replace with na and then drop these rows\n",
        "df = df.replace([np.inf, -np.inf], np.nan).dropna(axis=1) \n",
        "\n",
        "print('Sanity Check:',df.isnull().values.any())\n",
        "\n",
        "print('Rows Lost =',old_shape[0]-df.shape[0],'Columns Lost =',old_shape[1]-df.shape[1] )\n",
        "\n",
        "df_target=df['target']\n",
        "df=df.drop(columns=['target'])\n",
        "\n",
        "from sklearn import model_selection\n",
        "train_features,test_features,train_labels,test_labels=model_selection.train_test_split(df,df_target,test_size=0.3, random_state=17)\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler= MinMaxScaler()\n",
        "train_features = scaler.fit_transform(train_features)\n",
        "test_features = scaler.transform(test_features)\n",
        "\n",
        "from sklearn import preprocessing\n",
        "le = preprocessing.LabelEncoder()\n",
        "train_labels=le.fit_transform(train_labels)\n",
        "test_labels=le.transform(test_labels)\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import f_classif\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "\n",
        "\n",
        "#Remove all features which have a variance below 0.03\n",
        "constant_filter = VarianceThreshold(threshold=0.03)\n",
        "train_features0=constant_filter.fit_transform(train_features)\n",
        "test_features0=constant_filter.transform(test_features)\n",
        "\n",
        "\n",
        "FS_classifierRF = Pipeline([\n",
        "    (\"feature_selection\",SelectKBest(f_classif)),\n",
        "    (\"predictor\", RandomForestClassifier())\n",
        "    ])\n",
        "\n",
        "param_grid = {\"feature_selection__k\": [80,100,'all'], #10,20,30 --> then tried 5,7,9,10,12 8,9,10,11 \n",
        "              \"predictor__n_estimators\": [200,250],\n",
        "              \"predictor__criterion\": [\"gini\", \"log_loss\"],\n",
        "              \"predictor__max_features\":[\"sqrt\", \"log2\", None]\n",
        "              }\n",
        "              \n",
        "#cross validation\n",
        "FS_RF_gs = GridSearchCV(FS_classifierRF, param_grid, scoring=\"accuracy\",cv=10)\n",
        "\n",
        "# Run the GridSearchCV\n",
        "FS_RF_gs.fit(train_features0, train_labels)\n",
        "\n",
        "# Print the best parameters and the score\n",
        "FS_RF_gs.best_params_, FS_RF_gs.best_score_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtMBwvS9honm",
        "outputId": "3d45f711-77e0-4af5-c478-5322b58422d0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Na Values:  True\n",
            "Infinity Values: True\n",
            "Sanity Check: False\n",
            "Rows Lost = 0 Columns Lost = 16\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'feature_selection__k': 'all',\n",
              "  'predictor__criterion': 'log_loss',\n",
              "  'predictor__max_features': 'sqrt',\n",
              "  'predictor__n_estimators': 200},\n",
              " 0.9762923351158646)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FS_classifierRF.set_params(**FS_RF_gs.best_params_) \n",
        "FS_classifierRF.fit(train_features0, train_labels)\n",
        "accuracy_score(test_labels, FS_classifierRF.predict(test_features0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "261BQyXTizHx",
        "outputId": "c26abd0c-fc3e-4db6-fdb5-b2e0ea06648b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9726027397260274"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color=\"blue\">FOR GRADING ONLY</font>\n",
        "\n",
        "Save your best model into your github. And create a single code cell that loads it and evaluate it on the following test dataset: \n",
        "https://github.com/andvise/DataAnalyticsDatasets/blob/main/test_dataset.csv\n"
      ],
      "metadata": {
        "id": "Q01BjiiCJTR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from joblib import dump, load\n",
        "from io import BytesIO\n",
        "import requests\n",
        "dump(FS_classifierRF,'bestmodel.joblib')\n",
        "# INSERT YOUR MODEL'S URL\n",
        "mLink = 'URL_OF_YOUR_MODEL_SAVED_IN_YOUR_GITHUB_REPOSITORY?raw=true'\n",
        "# mfile = BytesIO(requests.get(mLink).content)\n",
        "# model = load(mfile)\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "IWx4lyuQI929"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}